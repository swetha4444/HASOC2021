{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "outputs": [],
      "metadata": {
        "id": "-oXjA06e4iVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "def remove_punch(text):\r\n",
        "    x = str(text)\r\n",
        "    for p in string.punctuation:\r\n",
        "        x = x.replace(p,'')\r\n",
        "    return x\r\n",
        "\r\n",
        "def deEmojify(text):\r\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           \"]+\", flags = re.UNICODE)\r\n",
        "    return regrex_pattern.sub(r'',text)\r\n",
        "\r\n",
        "def lower_tokens(tokens):\r\n",
        "    return  [w.lower() for w in tokens]\r\n",
        "\r\n",
        "def remove_stopwords(tokens):\r\n",
        "    stop = set(stopwords.words(\"english\"))\r\n",
        "    filtered_words = [word for word in tokens if word not in stop]\r\n",
        "    return \" \".join(filtered_words)\r\n",
        "\r\n",
        "def remove_names(text):\r\n",
        "    for word in text.split():\r\n",
        "        if word[0] == \"@\":\r\n",
        "            text = text.replace(word, \"\")\r\n",
        "    return text\r\n",
        "\r\n",
        "def remove_url(text):\r\n",
        "    result = re.sub(r\"http\\S+\", \"\", text)\r\n",
        "    result = re.sub(r\"https\\S+\", \"\", text)\r\n",
        "    return result\r\n",
        "\r\n",
        "\r\n",
        "def decode_unicode(text):\r\n",
        "    text = text.decode('utf-8')\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "def text_to_wordlist(text,stem_words=True):\r\n",
        "    # Clean the text\r\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\r\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\r\n",
        "    text = re.sub(r\"doesn't\", \"does not \", text)\r\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\r\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\r\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\r\n",
        "    text = re.sub(r\"n't\", \" not \", text)\r\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\r\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\r\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\r\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\r\n",
        "    text = re.sub(r\",\", \" \", text)\r\n",
        "    text = re.sub(r\"\\.\", \" \", text)\r\n",
        "    text = re.sub(r\"!\", \" ! \", text)\r\n",
        "    text = re.sub(r\"\\/\", \" \", text)\r\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\r\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\r\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\r\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\r\n",
        "    text = re.sub(r\"'\", \" \", text)\r\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\r\n",
        "    text = re.sub(r\":\", \" : \", text)\r\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\r\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\r\n",
        "    text = re.sub(r\" u s \", \" american \", text)\r\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\r\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\r\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\r\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\r\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\r\n",
        "    \r\n",
        "    # Optionally, shorten words to their stems\r\n",
        "    if stem_words:\r\n",
        "        text = text.split()\r\n",
        "        stemmer = SnowballStemmer('english')\r\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\r\n",
        "        text = \" \".join(stemmed_words)\r\n",
        "    \r\n",
        "    # remove consecutive letters to single letter at end\r\n",
        "    text = re.sub(r'(.)\\1+$', r'\\1', text)\r\n",
        "    \r\n",
        "    # Return a list of words\r\n",
        "    return(text)"
      ],
      "outputs": [],
      "metadata": {
        "id": "3-Y-d2TF44s2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "nltk.download('punkt')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHMxtpLx6k67",
        "outputId": "cbc14edf-a2cf-4c10-c60f-8a8691b78eba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "df = pd.read_csv('en_Hasoc2021_train.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "-4CX0GAt479y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "df['text_clean'] = df['text'].apply(lambda text:remove_names(text))\r\n",
        "df['text_clean'] = df['text_clean'].apply(lambda text:remove_url(text))\r\n",
        "df['text_clean'] = df['text_clean'].apply(lambda text:text_to_wordlist(text))"
      ],
      "outputs": [],
      "metadata": {
        "id": "QZkP-mqM-6Bw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "df['text_clean'] = df['text_clean'].apply(lambda text:deEmojify(text))\r\n",
        "df['text_clean'] = df['text_clean'].apply(lambda text:remove_punch(text))"
      ],
      "outputs": [],
      "metadata": {
        "id": "xG-OvUSl6KW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "tokens = [word_tokenize(tweet) for tweet in df['text_clean']]\r\n",
        "lower = [lower_tokens(token) for token in tokens]"
      ],
      "outputs": [],
      "metadata": {
        "id": "Dxfcsz6C7bjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "nltk.download('stopwords')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apyiK_Gy70om",
        "outputId": "73ea0944-f22b-45d3-b582-bb66051d0299"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "removed = []\r\n",
        "for tokens in lower:\r\n",
        "    removed.append(remove_stopwords(tokens))"
      ],
      "outputs": [],
      "metadata": {
        "id": "fws_JgDL8ZE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "df['text_clean'] = pd.Series(removed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                       _id  \\\n",
              "0        4986  60c5d6bf5659ea5e55defa2c   \n",
              "1        3394  60c5d6bf5659ea5e55def461   \n",
              "2        1310  60c5d6bf5659ea5e55defaad   \n",
              "3        3390  60c5d6bf5659ea5e55def419   \n",
              "4        4626  60c5d6bf5659ea5e55def7fa   \n",
              "\n",
              "                                                text task_1 task_2  \\\n",
              "0  @wealth if you made it through this &amp;&amp;...    HOF   PRFN   \n",
              "1  Technically that's still turning back the cloc...    HOF   OFFN   \n",
              "2  @VMBJP @BJP4Bengal @BJP4India @narendramodi @J...    NOT   NONE   \n",
              "3  @krtoprak_yigit Soldier of Japan Who has dick ...    HOF   OFFN   \n",
              "4  @blueheartedly You'd be better off asking who ...    HOF   OFFN   \n",
              "\n",
              "                                          text_clean  \n",
              "0  made amp amp onli abl start make money sustain...  \n",
              "1            technic still turn back clock dick head  \n",
              "2  govt stop think world media liber gang ani opt...  \n",
              "3                            soldier japan dick head  \n",
              "4         would better ask think sleazi shitbag lmao  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4986</td>\n",
              "      <td>60c5d6bf5659ea5e55defa2c</td>\n",
              "      <td>@wealth if you made it through this &amp;amp;&amp;amp;...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>made amp amp onli abl start make money sustain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3394</td>\n",
              "      <td>60c5d6bf5659ea5e55def461</td>\n",
              "      <td>Technically that's still turning back the cloc...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>technic still turn back clock dick head</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1310</td>\n",
              "      <td>60c5d6bf5659ea5e55defaad</td>\n",
              "      <td>@VMBJP @BJP4Bengal @BJP4India @narendramodi @J...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>govt stop think world media liber gang ani opt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3390</td>\n",
              "      <td>60c5d6bf5659ea5e55def419</td>\n",
              "      <td>@krtoprak_yigit Soldier of Japan Who has dick ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>soldier japan dick head</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4626</td>\n",
              "      <td>60c5d6bf5659ea5e55def7fa</td>\n",
              "      <td>@blueheartedly You'd be better off asking who ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>would better ask think sleazi shitbag lmao</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "DE3b38nS8r0W",
        "outputId": "a26b24b0-dca9-449d-c771-9951a207a51c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "df.to_csv('preprocess_data.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "VhyWQWtGAK4M"
      }
    }
  ]
}