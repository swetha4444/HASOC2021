{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blond-share",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "democratic-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-reynolds",
   "metadata": {},
   "source": [
    "### Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "statutory-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4986</td>\n",
       "      <td>60c5d6bf5659ea5e55defa2c</td>\n",
       "      <td>HOF</td>\n",
       "      <td>made amp amp onli abl start make money sustain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3394</td>\n",
       "      <td>60c5d6bf5659ea5e55def461</td>\n",
       "      <td>HOF</td>\n",
       "      <td>technic still turn back clock dick head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1310</td>\n",
       "      <td>60c5d6bf5659ea5e55defaad</td>\n",
       "      <td>NOT</td>\n",
       "      <td>govt stop think world media liber gang ani opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3390</td>\n",
       "      <td>60c5d6bf5659ea5e55def419</td>\n",
       "      <td>HOF</td>\n",
       "      <td>soldier japan dick head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4626</td>\n",
       "      <td>60c5d6bf5659ea5e55def7fa</td>\n",
       "      <td>HOF</td>\n",
       "      <td>would better ask think sleazi shitbag lmao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                       _id task_1  \\\n",
       "0          4986  60c5d6bf5659ea5e55defa2c    HOF   \n",
       "1          3394  60c5d6bf5659ea5e55def461    HOF   \n",
       "2          1310  60c5d6bf5659ea5e55defaad    NOT   \n",
       "3          3390  60c5d6bf5659ea5e55def419    HOF   \n",
       "4          4626  60c5d6bf5659ea5e55def7fa    HOF   \n",
       "\n",
       "                                          text_clean  \n",
       "0  made amp amp onli abl start make money sustain...  \n",
       "1            technic still turn back clock dick head  \n",
       "2  govt stop think world media liber gang ani opt...  \n",
       "3                            soldier japan dick head  \n",
       "4         would better ask think sleazi shitbag lmao  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preprocess_data.csv')\n",
    "data.drop(['task_2','Unnamed: 0','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-factor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "double-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neither-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  8255\n",
      "[[   0    0    0 ...  170    3  210]\n",
      " [   0    0    0 ...   72   54   73]\n",
      " [   0    0    0 ...    3   52   13]\n",
      " ...\n",
      " [   0    0    0 ...  817   45  156]\n",
      " [   0    0    0 ...  213   99   38]\n",
      " [   0    0    0 ... 1166  236   57]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_1']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proved-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "graduate-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compact-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2500, 256)         2113280   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,195,586\n",
      "Trainable params: 2,195,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "universal-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"hasoc_a_2.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "save_weights_only=False, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifteen-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.7146\n",
      "Epoch 00001: val_loss improved from inf to 0.48427, saving model to hasoc_a_2.h5\n",
      "103/103 [==============================] - 610s 6s/step - loss: 0.5608 - accuracy: 0.7146 - val_loss: 0.4843 - val_accuracy: 0.7782\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8298\n",
      "Epoch 00002: val_loss did not improve from 0.48427\n",
      "103/103 [==============================] - 609s 6s/step - loss: 0.3958 - accuracy: 0.8298 - val_loss: 0.4891 - val_accuracy: 0.7730\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8683\n",
      "Epoch 00003: val_loss did not improve from 0.48427\n",
      "103/103 [==============================] - 634s 6s/step - loss: 0.3196 - accuracy: 0.8683 - val_loss: 0.5265 - val_accuracy: 0.7608\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.8925\n",
      "Epoch 00004: val_loss did not improve from 0.48427\n",
      "103/103 [==============================] - 664s 6s/step - loss: 0.2621 - accuracy: 0.8925 - val_loss: 0.5968 - val_accuracy: 0.7452\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9274\n",
      "Epoch 00005: val_loss did not improve from 0.48427\n",
      "103/103 [==============================] - 621s 6s/step - loss: 0.1965 - accuracy: 0.9274 - val_loss: 0.6982 - val_accuracy: 0.7435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b47f04bb80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train ,batch_size = 32, epochs = 5 ,validation_data=(X_test,Y_test) , callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conceptual-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 9s 479ms/step - loss: 0.4843 - accuracy: 0.7782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48427435755729675, 0.7781628966331482]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a_2.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "facial-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "right-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in Y_true:\n",
    "    if i =='NOT':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "anticipated-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       380\n",
      "           1       0.71      0.59      0.65       197\n",
      "\n",
      "    accuracy                           0.78       577\n",
      "   macro avg       0.76      0.73      0.74       577\n",
      "weighted avg       0.77      0.78      0.77       577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-reduction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-foundation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
